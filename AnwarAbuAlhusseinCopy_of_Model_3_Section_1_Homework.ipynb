{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABUALHUSSEIN/mini-rag-system/blob/main/AnwarAbuAlhusseinCopy_of_Model_3_Section_1_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Homework: Build and Test a Mini RAG System from Scratch üß†\n",
        "\n",
        "> **üéØ Today's Goal**: Combine the knowledge from the first three lessons (Embeddings, Retrieval, Generation) to build a functional Retrieval-Augmented Generation (RAG) system from scratch. Then, test it with a self-assessment!"
      ],
      "metadata": {
        "id": "Fg7IRyVjzb74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers torch"
      ],
      "metadata": {
        "id": "NCUjET-gzbee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Part 1: The Retriever - Finding the Right Knowledge\n",
        "\n",
        "First, we'll set up our Retriever. Its job is to take a question and find the most relevant piece of text from our knowledge base.\n",
        "\n",
        "1.  **Load the Embedding Model** (`all-MiniLM-L6-v2`)\n",
        "2.  **Create our Knowledge Base**\n",
        "3.  **Encode Everything into Embeddings**\n",
        "4.  **Calculate Similarity** to find the best match"
      ],
      "metadata": {
        "id": "99P0U4tJzjq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "# 1. Load our embedding model\n",
        "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Create a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"\n",
        "]\n",
        "\n",
        "# 3. Encode our knowledge base into embeddings\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "print(f\"‚úÖ Retriever model loaded and knowledge base encoded with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "id": "CJHpc394zkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Part 2: The Generator - Extracting the Answer\n",
        "\n",
        "Now we set up our Generator. This model will take the question and the context found by the retriever and extract the exact answer from it."
      ],
      "metadata": {
        "id": "Plr7LaDsznpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our question-answering (generator) model\n",
        "generator = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "print(\"‚úÖ Generator (QA) model loaded.\")"
      ],
      "metadata": {
        "id": "sCHCTi1xzpto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Part 3: Testing our RAG System\n",
        "\n",
        "Time to put it all together! The function below will simulate a full RAG pipeline and grade itself against a predefined set of questions and answers.\n",
        "\n",
        "It will test two key things:\n",
        "1.  **Retrieval Accuracy**: Did we find the right document?\n",
        "2.  **Generation Accuracy**: Did we extract the correct answer from that document?"
      ],
      "metadata": {
        "id": "kJMFmEjgzsIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_assessment():\n",
        "    \"\"\"Runs a self-assessment of the RAG pipeline with multiple questions.\"\"\"\n",
        "\n",
        "    # Define our questions, expected context keywords, and expected answers\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the highest mountain?\",\n",
        "            \"expected_keyword\": \"Everest\",\n",
        "            \"expected_answer\": \"Mount Everest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which city is home to the Louvre museum?\",\n",
        "            \"expected_keyword\": \"France\",\n",
        "            \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What process do plants use for energy?\",\n",
        "            \"expected_keyword\": \"Photosynthesis\",\n",
        "            \"expected_answer\": \"Photosynthesis\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2 # 2 points per question (1 for retrieval, 1 for generation)\n",
        "\n",
        "    print(\"--- üöÄ Starting RAG System Assessment ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "\n",
        "        # --- 1. Retrieval Step ---\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "\n",
        "        print(f\"üîé  Retrieved Context: '{retrieved_context}'\")\n",
        "\n",
        "        # Check if the retrieval was correct\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"‚úÖ  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "\n",
        "        # --- 2. Generation Step ---\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "\n",
        "        print(f\"‚úçÔ∏è  Generated Answer: '{generated_answer}'\")\n",
        "\n",
        "        # Check if the generation was correct\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"‚úÖ  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    # --- Final Score ---\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üéâüéâüéâ Perfect! Your RAG system is working as expected!\")\n",
        "    elif score >= total / 2:\n",
        "        print(\"üëç Good job! The system is mostly correct.\")\n",
        "    else:\n",
        "        print(\"üîß The system ran into some issues. Review the steps and check the logic.\")\n",
        "\n",
        "# Run the assessment!\n",
        "run_rag_assessment()"
      ],
      "metadata": {
        "id": "OvP6XETQztxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STUDENT TASKS üßë‚Äçüíª\n",
        "\n",
        "Now it's your turn to be the AI engineer. Your tasks are to run, analyze, and extend the RAG system you've just built."
      ],
      "metadata": {
        "id": "bmsraUmd0ruq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Execute and Understand\n",
        "\n",
        "Your first task is to simply run all the cells above and carefully read the output of the final self-assessment.\n",
        "\n",
        "* **Observe the Score:** Did the system get a perfect score (6/6)?\n",
        "* **Analyze Each Step:** For each question, look at the \"Retrieved Context\" and the \"Generated Answer.\"\n",
        "    * Did the retriever find the correct piece of knowledge?\n",
        "    * Did the generator extract the right answer from that context?"
      ],
      "metadata": {
        "id": "RT5z7ZoE0trr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Task 1: Analysis and Observations\n",
        "\n",
        "After running the `run_rag_assessment()` function, I observed the following:\n",
        "\n",
        "*   **Final Score:** The system achieved a perfect score of **6/6**.\n",
        "\n",
        "*   **Step-by-Step Analysis:**\n",
        "    *   **Question 1 (\"What is the highest mountain?\"):**\n",
        "        *   ‚úÖ **Retrieval:** The system correctly retrieved the context: *\"Mount Everest is the highest mountain on Earth, located in the Himalayas.\"*\n",
        "        *   ‚úÖ **Generation:** The system correctly extracted the answer: *\"Mount Everest\"*.\n",
        "\n",
        "    *   **Question 2 (\"Which city is home to the Louvre museum?\"):**\n",
        "        *   ‚úÖ **Retrieval:** The system correctly retrieved the context: *\"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\"*\n",
        "        *   ‚úÖ **Generation:** The system correctly extracted the answer: *\"Paris\"*.\n",
        "\n",
        "    *   **Question 3 (\"What process do plants use for energy?\"):**\n",
        "        *   ‚úÖ **Retrieval:** The system correctly retrieved the context: *\"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"*\n",
        "        *   ‚úÖ **Generation:** The system correctly extracted the answer: *\"Photosynthesis\"*.\n",
        "\n",
        "*   **Conclusion:** The RAG system is working perfectly on the initial set of questions. Both the retriever and the generator components are performing as expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "ObbasKJL63KV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 (Challenge): Add a New Question\n",
        "\n",
        "Your second task is to test the system with a new question about the **existing knowledge**.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Copy the code from the cell below. It's the same assessment function as before, but with a new test question added.\n",
        "2.  Run the cell and see if the system can answer correctly. The score should now be out of 8."
      ],
      "metadata": {
        "id": "25iQeOoo6jTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Add a new question to the assessment function\n",
        "\n",
        "def run_rag_assessment_task_2():\n",
        "    \"\"\"Runs the assessment with the original questions PLUS one new question.\"\"\"\n",
        "\n",
        "    # Define our questions, including the original three and the new one\n",
        "    test_questions = [\n",
        "        # --- Original Questions ---\n",
        "        {\n",
        "            \"question\": \"What is the highest mountain?\",\n",
        "            \"expected_keyword\": \"Everest\",\n",
        "            \"expected_answer\": \"Mount Everest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which city is home to the Louvre museum?\",\n",
        "            \"expected_keyword\": \"France\",\n",
        "            \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What process do plants use for energy?\",\n",
        "            \"expected_keyword\": \"Photosynthesis\",\n",
        "            \"expected_answer\": \"Photosynthesis\"\n",
        "        },\n",
        "        # --- NEWLY ADDED QUESTION ---\n",
        "        {\n",
        "            \"question\": \"How long is the Great Wall of China?\",\n",
        "            \"expected_keyword\": \"China\",\n",
        "            \"expected_answer\": \"13,000 miles\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2 # Score is now out of 8\n",
        "\n",
        "    print(\"--- üöÄ Starting RAG System Assessment (Task 2) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "\n",
        "        # --- 1. Retrieval Step ---\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "\n",
        "        print(f\"üîé  Retrieved Context: '{retrieved_context}'\")\n",
        "\n",
        "        # Check if the retrieval was correct\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"‚úÖ  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "\n",
        "        # --- 2. Generation Step ---\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "\n",
        "        print(f\"‚úçÔ∏è  Generated Answer: '{generated_answer}'\")\n",
        "\n",
        "        # Check if the generation was correct\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"‚úÖ  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    # --- Final Score ---\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üéâüéâüéâ Perfect! Your RAG system handled the new question!\")\n",
        "    else:\n",
        "        print(\"üîß The system ran into some issues. Review the steps and check the logic.\")\n",
        "\n",
        "\n",
        "# Run the updated assessment\n",
        "run_rag_assessment_task_2()"
      ],
      "metadata": {
        "id": "RegjyYB69YDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 (Advanced Challenge): Add New Knowledge & Test It\n",
        "\n",
        "Your final and most important task is to **expand the RAG system's knowledge base** and then test it.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Add a new fact** to the `knowledge_base` in the code cell below.\n",
        "2.  **You must re-run this cell** to update the `knowledge_embeddings`! The system won't know about the new fact until you do.\n",
        "3.  Finally, run the last code cell, which has a new test question about the knowledge you just added."
      ],
      "metadata": {
        "id": "JNHQuccw7Duu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 1: Add a new sentence to the knowledge base\n",
        "\n",
        "# Start with the original knowledge base and add our new fact.\n",
        "knowledge_base_task_3 = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\",\n",
        "    # --- NEW KNOWLEDGE ADDED HERE ---\n",
        "    \"Mars is the fourth planet from the Sun and is often called the 'Red Planet'.\"\n",
        "]\n",
        "\n",
        "# Re-encode the updated knowledge base so the retriever can use it.\n",
        "knowledge_embeddings_task_3 = retriever_model.encode(knowledge_base_task_3, convert_to_tensor=True)\n",
        "\n",
        "print(f\"‚úÖ Knowledge base updated and re-encoded with {len(knowledge_base_task_3)} documents.\")"
      ],
      "metadata": {
        "id": "k_cP0eN8AT1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 2: Test your newly added knowledge\n",
        "\n",
        "def run_rag_assessment_task_3():\n",
        "    \"\"\"Tests the RAG system's ability to answer a question about the new knowledge.\"\"\"\n",
        "\n",
        "    test_questions = [\n",
        "        # --- NEW QUESTION FOR YOUR NEW KNOWLEDGE ---\n",
        "        {\n",
        "            \"question\": \"Which planet is known as the Red Planet?\",\n",
        "            \"expected_keyword\": \"Mars\",\n",
        "            \"expected_answer\": \"Mars\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- üöÄ Starting RAG System Assessment (Task 3) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "\n",
        "        # Use the updated embeddings from Task 3\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings_task_3)[0]\n",
        "\n",
        "        # Use the updated knowledge base from Task 3\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base_task_3[top_result_index]\n",
        "\n",
        "        print(f\"üîé  Retrieved Context: '{retrieved_context}'\")\n",
        "\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"‚úÖ  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "\n",
        "        print(f\"‚úçÔ∏è  Generated Answer: '{generated_answer}'\")\n",
        "\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"‚úÖ  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"‚ùå  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üèÜüèÜüèÜ Success! You have successfully extended the knowledge of your RAG system!\")\n",
        "\n",
        "# Run the final assessment\n",
        "run_rag_assessment_task_3()"
      ],
      "metadata": {
        "id": "9QJfJuEQBAdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}